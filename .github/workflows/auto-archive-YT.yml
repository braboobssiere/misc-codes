name: Auto-Download YT Video 

on:
  workflow_dispatch:
  # schedule: # use cron-job.org instead
  #   - cron: '20,50 * * * *'

concurrency:
  group: Auto-Download YT Video test
  cancel-in-progress: false

jobs:
  check-feed:
    name: Check Feed
    runs-on: ubuntu-latest
    outputs:
      should_process: ${{ steps.feed-parser.outputs.should_process }}
      urls: ${{ steps.feed-parser.outputs.urls }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install parsing dependencies
        run: pip install feedparser

      - name: Parse and Validate Feed
        id: feed-parser
        env:
          FEED_URL: "https://www.youtube.com/feeds/videos.xml?channel_id=UCp6993wxpyDPHUpavwDFqgg"
        run: |
          python3 <<EOF
          import os
          import json
          from datetime import datetime, timedelta, timezone
          import feedparser
          
          current_time = datetime.now(timezone.utc)
          threshold = current_time - timedelta(minutes=32)
          entries = []
          
          feed = feedparser.parse(os.environ['FEED_URL'])
          for entry in feed.entries:
              if not (updated := entry.get('updated')): continue
              try:
                  dt = datetime.fromisoformat(updated).astimezone(timezone.utc)
              except ValueError:
                  continue
              
              if threshold <= dt <= current_time:
                  link = next((link['href'] for link in entry.links 
                             if link.rel == 'alternate'), entry.link)
                  entries.append({'url': link, 'timestamp': dt.isoformat()})
 
          # sort by newest and get last valid entry
          entries.sort(key=lambda x: x['timestamp'], reverse=True)
          result = entries[:1] 
          print(f"Latest entry: {json.dumps(result, indent=2) if result else 'No entries found'}")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'should_process={bool(result)}\n')
              f.write(f'urls={json.dumps(result)}\n')    
          EOF
              
  process-video:
    name: Process Videos
    needs: check-feed
    if: ${{ needs.check-feed.outputs.should_process == 'true' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Dependencies
        run: |
          sudo apt-get install -y ffmpeg wireguard-tools curl
          python3 -m pip install --pre yt-dlp[default,curl-cffi]

      - name: VPN Setup 
        env: 
          PRIMARY_CONFIG: ${{ secrets.WIREGUARD_GRANDCENTRAL }}
          SECONDARY_CONFIG: ${{ secrets.WIREGUARD_TRADEWAR }}
          
        run: |
          sudo mkdir -p /etc/wireguard/
          WG_CONF="/etc/wireguard/wg0.conf"
          handle_vpn_attempt() {
            local config_content="$1"
            local label="$2"
            
            echo "$config_content" | sudo tee $WG_CONF > /dev/null
            sudo chmod 600 $WG_CONF

            for i in {1..3}; do
              if timeout 60 sudo wg-quick up wg0; then
                return 0
              fi
              echo "${label^} VPN attempt $i/3 failed"
              sleep 3
            done
            return 1
          }
          if ! handle_vpn_attempt "$PRIMARY_CONFIG" "primary"; then
            echo "::warning::Primary failed - Trying secondary"
            if ! handle_vpn_attempt "$SECONDARY_CONFIG" "secondary"; then
              echo "::error::All VPN configurations failed after 3 attempts each"
              exit 11
            fi
          fi
          if ! sudo wg show wg0 >/dev/null 2>&1; then
            echo "::error::VPN connection verification failed"
            exit 12
          fi

      - name: Downloads
        env:
          URL: ${{ fromJson(needs.check-feed.outputs.urls)[0].url }}
        continue-on-error: true
        run: |
          mkdir -p output
          # Live status check
          LIVE_STATUS=$(timeout 10s yt-dlp --print '%(live_status)s' "$URL" 2>/dev/null || echo "error")
          if [[ ! "$LIVE_STATUS" =~ ^(is_live|post_live)$ ]]; then
            echo "Skipping non-live stream (status: $LIVE_STATUS)"
            exit 13
          fi          
          yt-dlp -i --no-progress -S "+res:1080" --retry-sleep 3 \
          --windows-filenames --live-from-start --write-thumbnail \
          --embed-metadata --no-embed-info-json \
          -o "output/%(title).170B [%(id)s] (%(resolution)s).%(ext)s" \
          "$URL" >> output/logs.txt 2>&1

      - name: VPN Cleanup
        if: ${{ always() }}
        run: sudo wg-quick down wg0 || true

      - name: Verify output file 
        run: |
          # Count files in output directory (including hidden files)
          FILE_COUNT=$(find output/ -type f | wc -l)
          
          if [ $FILE_COUNT -lt 2 ]; then
            echo "Error: yt-dlp have a problem downloading video (found $FILE_COUNT)"
            exit 14
          fi

      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: downloaded-videos
          path: output/
          compression-level: 1
          if-no-files-found: error

      - name: Upload to gofile
        run: |
          UPLOAD_URL="https://upload.gofile.io/uploadfile"
          guest_token=""
          folder_id=""
          UPLOAD_LINKS=""
          FIRST_FILE=true

          while IFS= read -r -d '' file; do
            echo "‚ñ´Ô∏è Starting upload: $(basename "$file")"
            extra_args=()
            if [ "$FIRST_FILE" = false ]; then
              extra_args+=(-H "Authorization: Bearer $guest_token" -F "folderId=$folder_id")
            fi
            RESPONSE=$(curl -s -X POST -F "file=@\"$file\"" "${extra_args[@]}" "$UPLOAD_URL")
            PYTHON_OUTPUT=$(python3 <<EOF
          import json, sys
          try:
              data = json.loads('''$RESPONSE''')
              if "$FIRST_FILE" == "true":
                  guest_token = data['data']['guestToken']
                  folder_id = data['data']['parentFolder']
                  print(f"{guest_token}\t{folder_id}")
              else:
                  print("-\t-")          
              status = 'ok' if data['status'] == 'ok' else 'error'
              result_data = data.get('data', {}).get('downloadPage', '') if status == 'ok' else data.get('data', '')
              print(f"{status}\t{result_data}")
          except Exception as e:
              print(f"error\tJSON parsing failed: {str(e)}")
              sys.exit(1)
          EOF
            )

            {
              read -r token_part folder_part
              read -r status_type result_data
            } <<< "$PYTHON_OUTPUT"

            if [ "$FIRST_FILE" = true ]; then
              guest_token="$token_part"
              folder_id="$folder_part"
              FIRST_FILE=false
              echo "üîë Created folder ID: $folder_id"
            fi

            if [ "$status_type" != "ok" ]; then
              echo "::error file=$file::Upload failed: $result_data"
            else
              LINK="$result_data"
              UPLOAD_LINKS+="- $(basename "$file")\n"
              echo "‚úÖ Success: $LINK"
            fi
          done < <(find output/ -type f -print0)

          echo "### File Uploads: $LINK" >> $GITHUB_STEP_SUMMARY
          echo -e "\n$UPLOAD_LINKS" >> $GITHUB_STEP_SUMMARY

          curl -X POST \
          -H "Content-Type: application/json" \
          -d "{\"content\":\"File Uploads: $LINK \n$UPLOAD_LINKS\"}" \
          "${{ secrets.DISCORD_WEBHOOK }}"
