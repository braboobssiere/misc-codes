name: Auto-Download YT Video 

on:
  workflow_dispatch:
  # schedule: 
  #   - cron: '20,50 * * * *'
  # use cron-job.org instead

concurrency:
  group: Auto-Download YT Video
  cancel-in-progress: false

jobs:
  check-feed:
    name: Parse Feed & Poll for Live
    runs-on: ubuntu-latest
    outputs:
      should_process: ${{ steps.check.outputs.should_process }}
      urls:           ${{ steps.check.outputs.urls }}

    steps:

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install feedparser requests

      - name: Parse feed, check live & emit outputs
        id: check
        env:
          FEED_URL: "https://www.youtube.com/feeds/videos.xml?channel_id=UCoiEtD4v1qMAqHV5MDI5Qpg" # 9arm
          YT_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        run: |
          python <<'EOF'
          import os, json, time
          from datetime import datetime, timedelta, timezone
          import feedparser, requests

          # 1) Parse feed for last xx minutes
          now    = datetime.now(timezone.utc)
          cutoff = now - timedelta(minutes=65)
          feed   = feedparser.parse(os.environ['FEED_URL'])

          recent = []
          for e in feed.entries:
              updated = e.get('updated')
              if not updated:
                  continue
              try:
                  dt = datetime.fromisoformat(updated).astimezone(timezone.utc)
              except ValueError:
                  continue
              if cutoff <= dt <= now:
                  link = next((l['href'] for l in e.links if l.get('rel')=='alternate'), e.link)
                  recent.append({'url': link, 'timestamp': dt.isoformat()})

          # sort newest-first
          recent.sort(key=lambda x: x['timestamp'], reverse=True)

          # 2) Poll YouTube API for live status
          api_key     = os.environ['YT_API_KEY']
          live_entry  = None

          for entry in recent:
              url = entry['url']
              # extract video ID
              vid = None
              if 'v=' in url:
                  vid = url.split('v=')[1].split('&')[0]
              elif 'youtu.be/' in url:
                  vid = url.split('youtu.be/')[1].split('?')[0]
              if not vid:
                  continue

              resp = requests.get(
                  "https://www.googleapis.com/youtube/v3/videos",
                  params={"part":"snippet","id":vid,"key":api_key},
                  timeout=10
              )
              data = resp.json().get("items", [])
              if data and data[0]["snippet"].get("liveBroadcastContent") == "live":
                  live_entry = entry
                  break

              time.sleep(3)

          # 3) Prepare outputs
          should = live_entry is not None
          urls   = [live_entry] if should else []

          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              out.write(f"should_process={str(should).lower()}\n")
              out.write("urls=" + json.dumps(urls) + "\n")
          EOF

  process-video:
    name: Process Videos
    needs: check-feed
    if: ${{ needs.check-feed.outputs.should_process == 'true' }}
    runs-on: ubuntu-latest
    steps:

      - name: Set weekly cache key
        id: weekly-key
        run: |
          WEEK=$(date +"%Y.%U")
          echo "week=${WEEK}" >> $GITHUB_OUTPUT
          
      - name: Cache APT packages
        uses: awalsh128/cache-apt-pkgs-action@latest
        with:
          packages: ffmpeg wireguard-tools lftp
          version: ${{ steps.weekly-key.outputs.week }}

      - name: Prepare bgutil
        run: |
          # Fetch the latest version tag from the repository
          latest_version=$(git ls-remote --tags https://github.com/Brainicism/bgutil-ytdlp-pot-provider.git | awk -F/ '{print $3}' | sort -V | tail -n 1)
          echo "Latest version: $latest_version"
          cd ~
          git clone --single-branch --branch $latest_version https://github.com/Brainicism/bgutil-ytdlp-pot-provider.git
          cd bgutil-ytdlp-pot-provider/server/
          npm install
          npx tsc

      - name: Setup Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg wireguard-tools lftp
          echo -e "yt-dlp[default,curl-cffi]\nbgutil-ytdlp-pot-provider" > requirements.txt
          
      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}-${{ steps.weekly-key.outputs.week }}
          
      - name: Install yt-dlp and bgutil
        run: |
          python3 -m pip install --pre yt-dlp[default,curl-cffi]
          python3 -m pip install -U bgutil-ytdlp-pot-provider

      - name: VPN Setup 
        env: 
          PRIMARY_CONFIG: ${{ secrets.WIREGUARD_GRANDCENTRAL }}
          SECONDARY_CONFIG: ${{ secrets.WIREGUARD_TRADEWAR }}
          
        run: |
          sudo mkdir -p /etc/wireguard/
          WG_CONF="/etc/wireguard/wg0.conf"
          handle_vpn_attempt() {
            local config_content="$1"
            local label="$2"
            
            echo "$config_content" | sudo tee $WG_CONF > /dev/null
            sudo chmod 600 $WG_CONF

            for i in {1..3}; do
              if timeout 60 sudo wg-quick up wg0; then
                return 0
              fi
              echo "${label^} VPN attempt $i/3 failed"
              sleep 3
            done
            return 1
          }
          if ! handle_vpn_attempt "$PRIMARY_CONFIG" "primary"; then
            echo "::warning::Primary failed - Trying secondary"
            if ! handle_vpn_attempt "$SECONDARY_CONFIG" "secondary"; then
              echo "::error::All VPN configurations failed after 3 attempts each"
              exit 11
            fi
          fi
          if ! sudo wg show wg0 >/dev/null 2>&1; then
            echo "::error::VPN connection verification failed"
            exit 12
          fi

      - name: Downloads
        env:
          URL: ${{ fromJson(needs.check-feed.outputs.urls)[0].url }}
        continue-on-error: true
        run: |
          mkdir -p output
          echo "${{ secrets.YT_COOKIES }}" >> yt_cookies.txt
          yt-dlp -i --no-progress -S "+res:1080" --retry-sleep 3 \
          --windows-filenames --live-from-start --write-thumbnail --js-runtimes node \
          --embed-metadata --no-embed-info-json --write-info-json --wait-for-video 300-600 \
          --extractor-args "youtube:player-client=default" --cookies yt_cookies.txt \
          -o "output/%(title).170B [%(id)s] (%(resolution)s).%(ext)s" \
          "$URL" 2>&1 | tee >(awk '{ print strftime("[%Y-%m-%d %H:%M:%S UTC]"), $0 }' >> output/logs.txt)
          # removed --cookies yt_cookies.txt 

      - name: VPN Cleanup
        if: ${{ always() }}
        run: sudo wg-quick down wg0 || true

      - name: Verify output file 
        run: |
          # Remove fragments if exist
          if ls output/*.part 1> /dev/null 2>&1; then
            rm output/*.part
          fi
          # Count files in output directory (including hidden files)
          FILE_COUNT=$(find output/ -type f | wc -l)
          
          if [ $FILE_COUNT -lt 2 ]; then
            echo "Error: yt-dlp have a problem downloading video (found $FILE_COUNT)"
            exit 14
          fi

      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: 9arm-video
          path: output
          compression-level: 1
          if-no-files-found: error
          retention-days: 7

      - name: Upload files
        continue-on-error: true
        run: |
          UPLOAD_URL="https://upload.gofile.io/uploadfile"
          guest_token=""
          folder_id=""
          UPLOAD_LINKS=""
          FIRST_FILE=true

          while IFS= read -r -d '' file; do
            echo "▫️ Starting upload: $(basename "$file")"
            extra_args=()
            if [ "$FIRST_FILE" = false ]; then
              extra_args+=(-H "Authorization: Bearer $guest_token" -F "folderId=$folder_id")
            fi
            
            RESPONSE=$(curl -s -X POST -F "file=@\"$file\"" "${extra_args[@]}" "$UPLOAD_URL")

            PYTHON_OUTPUT=$(python3 <<EOF
          import json, sys
          try:
              data = json.loads('''$RESPONSE''')
              if "$FIRST_FILE" == "true":
                  guest_token = data['data']['guestToken']
                  folder_id = data['data']['parentFolder']
                  print(f"{guest_token}\t{folder_id}")
              else:
                  print("-\t-")          
              status = 'ok' if data['status'] == 'ok' else 'error'
              result_data = data.get('data', {}).get('downloadPage', '') if status == 'ok' else data.get('data', '')
              print(f"{status}\t{result_data}")
          except Exception as e:
              print(f"error\tJSON parsing failed: {str(e)}")
              sys.exit(1)
          EOF
            )

            {
              read -r token_part folder_part
              read -r status_type result_data
            } <<< "$PYTHON_OUTPUT"

            if [ "$FIRST_FILE" = true ]; then
              guest_token="$token_part"
              folder_id="$folder_part"
              FIRST_FILE=false
              echo "GUEST_TOKEN=$guest_token" >> $GITHUB_ENV
              echo "FOLDER_ID=$folder_id" >> $GITHUB_ENV
              echo "Created folder ID"
            fi

            if [ "$status_type" != "ok" ]; then
              echo "::error file=$file::Upload failed: $result_data"
            else
              LINK="$result_data"
              UPLOAD_LINKS+="- $(basename "$file")\n"
              echo "Success: $LINK"
            fi
          done < <(find output/ -type f -print0)

          echo "### File Uploads: $LINK" >> $GITHUB_STEP_SUMMARY
          echo -e "\n$UPLOAD_LINKS" >> $GITHUB_STEP_SUMMARY

          curl -X POST \
            -H "Content-Type: application/json" \
            -d "{\"content\":\"File Uploads: $LINK \n$UPLOAD_LINKS\"}" \
            "${{ secrets.DISCORD_WEBHOOK }}"

      - name: Upload to FTP 
        continue-on-error: true
        run: |
          mkdir -p ftp_upload
          find output -type f \( -name "*.mp4" -o -name "*.mkv" -o -name "*.ts" -o -name "*.webm" \) \
            -exec mv -- {} ftp_upload/ \;
          if [ "$(ls -A ftp_upload)" ]; then
            upload_server=$(curl -s "https://api.vidara.so/v1/upload/server?api_key=${{ secrets.STREAMIX_API_KEY }}" | jq -r '.result.upload_server')
            for f in ftp_upload/*; do
              [ -f "$f" ] && curl -X POST -o /dev/null \
              -F "api_key=${{ secrets.STREAMIX_API_KEY }}" -F "file=@$f" \
              "$upload_server" \
              && echo "Uploaded: $(basename "$f")"
            done
          else
                echo "No files found. Skipping FTP upload."
          fi

      - name: Upload to FTP (Server 2)
        continue-on-error: true
        run: |
          UPLOAD_SERVER=$(curl -s "https://vidsonic.net/api/v1/getUploadSrv?key=${{ secrets.vidsonic_api_key }}" | jq -r '.server')
          for f in ftp_upload/*; do
            [ -f "$f" ] && curl -X POST -o /dev/null $UPLOAD_SERVER \
              -F "apiKey=${{ secrets.vidsonic_api_key }}" -F "video=@$f" \
              && echo "Uploaded: $(basename "$f")"
          done
