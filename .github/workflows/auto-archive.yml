name: Scheduled Auto-Archive

on:
  workflow_dispatch:
  schedule:
    - cron: '35 * * * *'

jobs:
  process_feed_and_download:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Environment
        run: |
          sudo apt-get update
          PACKAGES="ffmpeg python3-pip wireguard-tools jq curl"
          sudo apt-get install -y $PACKAGES
          python3 -m pip install --upgrade pip --pre yt-dlp[default,curl-cffi]

      - name: Parse Atom Feed
        id: feed-parser
        run: |
          import xml.etree.ElementTree as ET
          import requests
          import re
          from datetime import datetime, timedelta
          import os

          feed_url = "https://raw.githubusercontent.com/braboobssiere/holedex-song-list/main/feeds/holodex.atom"
          response = requests.get(feed_url)
          root = ET.fromstring(response.content)

          ns = {'atom': 'http://www.w3.org/2005/Atom'}
          current_time = datetime.utcnow()
          time_threshold = current_time - timedelta(minutes=59)
          matched_entries = []

          for entry in root.findall('atom:entry', ns):
              # Check title condition
              title = entry.find('atom:title', ns).text
              if 'unarchive' not in title.lower():
                  continue
              
              # Extract timestamp from summary
              summary = entry.find('atom:summary', ns).text
              timestamp_match = re.search(r't:(\d+)', summary)
              if not timestamp_match:
                  continue
                  
              entry_time = datetime.utcfromtimestamp(int(timestamp_match.group(1)))
              
              # Time window check
              if time_threshold <= entry_time <= current_time:
                  link = entry.find("atom:link[@rel='alternate']", ns).attrib['href']
                  matched_entries.append((entry_time, link))

          # Select latest entry if multiple matches
          output_url = ""
          if matched_entries:
              # Sort by timestamp descending and pick latest
              matched_entries.sort(reverse=True, key=lambda x: x[0])
              output_url = matched_entries[0][1]
              print(f"::set-output name=should_process::true")
              print(f"::set-output name=url::{output_url}")
          else:
              print("::set-output name=should_process::false")
              
        shell: python
        env:
          TZ: UTC  # Ensure consistent timezone

      - name: VPN Setup (TradeWar)
        if: ${{ steps.feed-parser.outputs.should_process == 'true' }}
        env:
          TradeWar_CONFIG: ${{ secrets.WIREGUARD_TRADEWAR }}
        run: |
          sudo mkdir -p /etc/wireguard/
          echo "$TradeWar_CONFIG" | sudo tee /etc/wireguard/wg0.conf > /dev/null
          sudo chmod 600 /etc/wireguard/wg0.conf

          for i in {1..5}; do
              sudo wg-quick up wg0
              sleep 1 
              if sudo wg show wg0 >/dev/null 2>&1; then
                  echo "VPN connected successfully on attempt $i"
                  break
              else
                  echo "VPN connection failed (attempt $i/5)"
                  sudo wg-quick down wg0 >/dev/null 2>&1
                  [ $i -lt 5 ] && sleep 1
              fi
          done

          if ! sudo wg show wg0 >/dev/null 2>&1; then
              echo "::error::VPN failed to connect after 5 attempts"
              exit 1
          fi

      - name: Download and Process Video
        if: ${{ steps.feed-parser.outputs.should_process == 'true' }}
        continue-on-error: true
        run: |
          mkdir -p output
          URL="${{ steps.feed-parser.outputs.url }}"
          
          yt-dlp -i --no-progress \
            -S "+res:1080,+vcodec:av01" --windows-filenames \
            -o "output/%(title).170B [%(id)s] (%(resolution)s).%(ext)s" \
            --live-from-start --embed-metadata "${URL}"

      - name: VPN Cleanup
        if: ${{ always() && steps.feed-parser.outputs.should_process == 'true' }}
        run: |
          sudo wg-quick down wg0 || true

      - name: Upload Artifact
        if: ${{ steps.feed-parser.outputs.should_process == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: downloaded-video
          path: output
          compression-level: 1
          if-no-files-found: error

      - name: Upload Files to Cloud Storage
        if: ${{ steps.feed-parser.outputs.should_process == 'true' }}
        run: |
          UPLOAD_URL="https://upload.gofile.io/uploadfile"
          UPLOAD_LINKS=""
          guest_token=""
          folder_id=""

          upload_file() {
            local file=$1
            shift
            local extra_args=("$@")

            RESPONSE=$(curl -s -X POST -F "file=@\"$file\"" "${extra_args[@]}" "$UPLOAD_URL")
            STATUS=$(echo "$RESPONSE" | jq -r '.status')
            if [ "$STATUS" != "ok" ]; then
              ERROR=$(echo "$RESPONSE" | jq -r '.data // .status')
              echo "::error::Upload failed for $file: $ERROR"
              exit 1
            fi
            echo "$RESPONSE"
          }

          for file in output/*; do
            echo "Uploading $(basename "$file")..."
            extra_args=()
            if [ -n "$guest_token" ] && [ "$guest_token" != "null" ] && 
               [ -n "$folder_id" ] && [ "$folder_id" != "null" ]; then
              echo "::add-mask::$guest_token"
              echo "::add-mask::$folder_id"
              extra_args=(-H "Authorization: Bearer $guest_token" -F "folderId=$folder_id")
            fi

            RESPONSE=$(upload_file "$file" "${extra_args[@]}")

            if [ -z "$guest_token" ]; then
              guest_token=$(echo "$RESPONSE" | jq -r '.data.guestToken')
              echo "::add-mask::$guest_token"
            fi
            if [ -z "$folder_id" ]; then
              folder_id=$(echo "$RESPONSE" | jq -r '.data.parentFolder')
              echo "::add-mask::$folder_id"
            fi

            LINK=$(echo "$RESPONSE" | jq -r '.data.downloadPage')
            UPLOAD_LINKS+="- [$(basename "$file")]($LINK)\n"
          done

          FILE_COUNT=$(echo -e "$UPLOAD_LINKS" | grep -c '^-')
          SUMMARY="### 📁 Upload Results\n**Files uploaded:** $FILE_COUNT"

          # Append the upload summary to GitHub Step Summary
          echo -e "$SUMMARY\n\n$UPLOAD_LINKS" >> $GITHUB_STEP_SUMMARY
          echo "links=$(echo -e "$UPLOAD_LINKS" | base64 -w 0)" >> $GITHUB_OUTPUT
